{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP-basics.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgullate/Curso-IE/blob/master/NLP_basics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzkIXO5YO4el",
        "colab_type": "text"
      },
      "source": [
        "## Tokenization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q9x_0XJTObQl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lXpjPUKMOdjP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "text = \"This is Andrew's text, isn't it?\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PU40ZxmbOu5U",
        "colab_type": "text"
      },
      "source": [
        "See the result of tokenizing this text when we use three different tokenizers from the `nltk` library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfGxGRcmOjkb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kMbExKq4OnF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GxGMj-W4OqNW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenizer = nltk.tokenize.WordPunctTokenizer()\n",
        "tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vDC-ypwNOtjR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2gYpspKCO-Kg",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization and Stemming"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TfginnsuPBKh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "text = \"feet wolves cats talked\"\n",
        "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
        "tokens = tokenizer.tokenize(text)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10-mj3WrPNLZ",
        "colab_type": "text"
      },
      "source": [
        "Stemming extracts the root of the word, thus pruning plurals and derivations. It can produce \"non-words\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rh7iufCxPEzJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = nltk.stem.PorterStemmer()\n",
        "\" \".join(stemmer.stem(token) for token in tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xg2CEtPJPJII",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stemmer = nltk.stem.WordNetLemmatizer()\n",
        "\" \".join(stemmer.lemmatize(token) for token in tokens)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_HY5p9hPthh",
        "colab_type": "text"
      },
      "source": [
        "## Bag of Words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmp0mMGoRNWm",
        "colab_type": "text"
      },
      "source": [
        "Let us create a Bag of Words encoding of a set of texts. Can control the minimum word frequency, the size of N-grams, etc."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H6lar95yPvOf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "\n",
        "import pandas as pd\n",
        "texts = [\n",
        "    \"good movie fellas\", \"not a good movie\", \"did not like\", \n",
        "    \"i like it\", \"good one\"\n",
        "]\n",
        "# using default tokenizer in TfidfVectorizer\n",
        "BoW = CountVectorizer(min_df=1, ngram_range=(1, 1))\n",
        "features = BoW.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=BoW.get_feature_names()\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XchCfq3HRbqL",
        "colab_type": "text"
      },
      "source": [
        "When we use the tf-idf weights, we see that frequent terms in all documents get downweighted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uPII7GsRQCWg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tfidf = TfidfVectorizer(min_df=1, ngram_range=(1, 1))\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names()\n",
        ")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZvCDkvgQdF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
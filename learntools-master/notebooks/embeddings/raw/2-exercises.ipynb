{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise (Matrix Factorization)\n",
    "\n",
    "In this lesson, we'll reuse the model we trained in [the tutorial](#$TUTORIAL_URL(2)$). To get started, run the setup cell below to import the libraries we'll be using, load our data into Dataframes, and load a serialized version of the model we trained earlier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from learntools.core import binder; binder.bind(globals())\n",
    "from learntools.embeddings.ex2_factorization import *\n",
    "\n",
    "#_RM_\n",
    "input_dir = '../input/movielens_preprocessed'\n",
    "#_UNCOMMENT_\n",
    "#input_dir = '../input/movielens-preprocessing'\n",
    "df = pd.read_csv(os.path.join(input_dir, 'rating.csv'), usecols=['userId', 'movieId', 'rating', 'y'])\n",
    "movies = pd.read_csv(os.path.join(input_dir, 'movie.csv'), index_col=0)\n",
    "\n",
    "#_RM_\n",
    "model_dir = '.'\n",
    "#_UNCOMMENT_\n",
    "#model_dir = '../input/matrix-factorization'\n",
    "model_fname = 'factorization_model.h5'\n",
    "model_path = os.path.join(model_dir, model_fname)\n",
    "model = keras.models.load_model(model_path)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Generating Recommendations\n",
    "\n",
    "At the end of [the first lesson where we built an embedding model](#$TUTORIAL_URL(1)$), I showed how we could use our model to predict the ratings a particular user would give to some set of movies.\n",
    "\n",
    "For reference, here's a (slightly modified) copy of that code where we calculated predicted ratings for 5 specific movies:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Id of the user for whom we're predicting ratings\n",
    "uid = 26556\n",
    "candidate_movies = movies[\n",
    "    movies.title.str.contains('Naked Gun')\n",
    "    | (movies.title == 'The Sisterhood of the Traveling Pants')\n",
    "    | (movies.title == 'Lilo & Stitch')\n",
    "].copy()\n",
    "\n",
    "preds = model.predict([\n",
    "    [uid] * len(candidate_movies), # User ids \n",
    "    candidate_movies.index, # Movie ids\n",
    "])\n",
    "# Because our model was trained on a 'centered' version of rating (subtracting the mean, so that\n",
    "# the target variable had mean 0), to get the predicted star rating on the original scale, we need\n",
    "# to add the mean back in.\n",
    "row0 = df.iloc[0]\n",
    "offset = row0.rating - row0.y\n",
    "candidate_movies['predicted_rating'] = preds + offset\n",
    "candidate_movies.head()[ ['movieId', 'title', 'predicted_rating'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suppose we're interested in the somewhat more open-ended problem of **generating recommendations**. i.e. given some user ID and some number `k`, we need to generate a list of `k` movies we think the user will enjoy.\n",
    "\n",
    "The most straightforward way to do this would be to calculate the predicted rating this user would assign for *every movie in the dataset*, then take the movies with the `k` highest predictions.\n",
    "\n",
    "In the code cell below, fill in the body of the `recommend` function to do this. (Hint: you may want to use the cell above as a reference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(model, user_id, n=5):\n",
    "    \"\"\"Return a DataFrame with the n most highly recommended movies for the user with the\n",
    "    given id. (Where most highly recommended means having the highest predicted ratings \n",
    "    according to the given model).\n",
    "    The returned DataFrame should have a column for movieId and predicted_rating (it may also have\n",
    "    other columns).\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part1.hint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part1.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Correct (solution code)\n",
    "def recommend(model, user_id, n=5):\n",
    "    all_movie_ids = movies.index\n",
    "    preds = model.predict([\n",
    "        np.repeat(uid, len(all_movie_ids)),\n",
    "        all_movie_ids,\n",
    "    ])\n",
    "    # Add back the offset calculated earlier, to 'uncenter' the ratings, and get back to a [0.5, 5] scale.\n",
    "    movies.loc[all_movie_ids, 'predicted_rating'] = preds + offset\n",
    "    reccs = movies.sort_values(by='predicted_rating', ascending=False).head(n)\n",
    "    return reccs\n",
    "part1.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Sanity check\n",
    "\n",
    "Run the code cell below to get our most highly recommended movies for user #26556."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend(model, 26556)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do these recommendations seem sensible? If you'd like a reminder of user 26556's tastes, run the cell below to see all their ratings (in descending order)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = 26556\n",
    "user_ratings = df[df.userId==uid]\n",
    "movie_cols = ['movieId', 'title', 'genres', 'year', 'n_ratings', 'mean_rating']\n",
    "user_ratings.sort_values(by='rating', ascending=False).merge(movies[movie_cols], on='movieId')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Review our top-recommended movies. Are they reasonable? If not, where did we go wrong? You may also find it interesting to look at:\n",
    "- The metadata associated with the top-recommended movies\n",
    "- The 'least-recommended' movies (the ones with the lowest predicted scores)\n",
    "- The actual predicted rating values.\n",
    "\n",
    "Once you have an opinion, uncomment the cell below to see if we're in agreement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part2.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: How are we going to fix this mess?\n",
    "\n",
    "How can we improve the problem with our recommendations that we identified in Part 2? This could involve changing our model's structure, our training procedure, or our procedure for generating recommendations given a model.\n",
    "\n",
    "Give it some thought, then uncomment the cell below to compare notes with me. (If you have no idea, that's totally fine!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Fixing our obscure recommendation problem (thresholding)\n",
    "\n",
    "Fill in the code cell below to implement the `recommend_nonobscure` function, which will recommend the best movies which have at least some minimum number of ratings. (You may wish to modify the code you wrote in `recommend`, or even call `recommend` as a subroutine)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_nonobscure(model, user_id, n=5, min_ratings=1000):\n",
    "    \"\"\"Return a DataFrame with the n movies which the given model assigns the highest \n",
    "    predicted ratings for the given user, *limited to movies with at least the given\n",
    "    threshold of ratings*.\n",
    "    \"\"\"\n",
    "    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part4.hint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part4.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Correct (solution code)\n",
    "def recommend_nonobscure(model, user_id, n=5, min_ratings=1000):\n",
    "    # Add predicted_rating column if we haven't already done so.\n",
    "    if 'predicted_rating' not in movies.columns:\n",
    "        all_movie_ids = df.movieId.unique()\n",
    "        preds = model.predict([\n",
    "            np.repeat(uid, len(all_movie_ids)),\n",
    "            all_movie_ids,\n",
    "        ])\n",
    "        # Add back the offset calculated earlier, to 'uncenter' the ratings, and get back to a [0.5, 5] scale.\n",
    "        movies.loc[all_movie_ids, 'predicted_rating'] = preds + offset\n",
    "\n",
    "    nonobscure_movie_ids = movies.index[movies.n_ratings >= min_ratings]\n",
    "    return movies.loc[nonobscure_movie_ids].sort_values(by='predicted_rating', ascending=False).head(n)\n",
    "part4.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to take a look at our new recommended movies. Did this fix our problem? Do we get better results with a different threshold?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommend_nonobscure(model, uid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: A whirlwind introduction to L2 regularization\n",
    "\n",
    "> If you're already familiar with L2 regularization, feel free to skip this part.\n",
    "\n",
    "We train our model by minimizing a loss function. In this case, that's the squared difference between our model's predicted rating and the actual rating. L2 regularization adds another term to our model's loss function - a \"weight penalty\". Now our model must balance making accurate predictions while keeping embedding weights not too big.\n",
    "\n",
    "We call this a form of regularization, meaning it's expected to reduce overfitting to the training set. How? And what does this have to do with our obscure recommendation problem?\n",
    "\n",
    "Even if a movie has only a single rating in the dataset, our model will, in the absence of regularization, try to move its embedding around to match that one rating. However, if the model has a budget for movie weights, it's not very efficient to spend it on improving the accuracy of one rating out of 20,000,000. Popular movies will be worth assigning large weights. Obscure movies should have weights close to 0.\n",
    "\n",
    "Does this seem sensible? Test your understanding: What can we say about our model's output/predicted rating for a movie whose embedding vector is all zeros? i.e. `[0, 0, 0, 0, 0, 0, 0, 0]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part5.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Fixing our obscure recommendation problem (regularization)\n",
    "\n",
    "The code below is identical to the code used to create the model we've been using in this exercise, except we've added L2 regularization to our embeddings (by specifying a value for the keyword argument `embeddings_regularizer` when creating our Embedding layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_embedding_size = user_embedding_size = 8\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "\n",
    "movie_r12n = keras.regularizers.l2(1e-6)\n",
    "user_r12n = keras.regularizers.l2(1e-7)\n",
    "user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size,\n",
    "                                       embeddings_regularizer=user_r12n,\n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                        embeddings_regularizer=movie_r12n,\n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "\n",
    "dotted = keras.layers.Dot(2)([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(dotted)\n",
    "\n",
    "l2_model = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training this model for a decent number of iterations takes around 15 minutes, so to save some time, I have an already trained model you can load from disk by running the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_RM_\n",
    "model_dir = '.'\n",
    "#_UNCOMMENT_\n",
    "#model_dir = '../input/regularized-movielens-factorization-model'\n",
    "model_fname = 'movie_svd_model_8_r12n.h5'\n",
    "model_path = os.path.join(model_dir, model_fname)\n",
    "l2_model = keras.models.load_model(model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(If you're curious, you can check out the kernel where I trained this model [here](https://www.kaggle.com/colinmorris/regularized-movielens-factorization-model). You may notice that, aside from whether the addition of regularization improves the subjective quality of our recommendations, it already has the benefit of improving our validation error, by reducing overfitting.)\n",
    "\n",
    "Try using the code you wrote in part 1 to generate recommendations using this model. How do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the recommend() function you wrote earlier to get the 5 best recommended movies\n",
    "# for user 26556, and assign them to the variable l2_reccs.\n",
    "l2_reccs = []\n",
    "l2_reccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part6.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Bad solution (wrong model)\n",
    "l2_reccs = recommend(model, uid)\n",
    "part6.check()\n",
    "l2_reccs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Correct (soln code)\n",
    "l2_reccs = recommend(l2_model, 26556)\n",
    "part6.check()\n",
    "l2_reccs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What do you think this model's predicted scores will look like for the 'obscure' movies that our earlier model highly recommended? \n",
    "\n",
    "Think about it, then run the cell below to see if you're right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uid = 26556\n",
    "obscure_reccs = recommend(model, uid)\n",
    "obscure_mids = obscure_reccs.index\n",
    "preds = l2_model.predict([\n",
    "    np.repeat(uid, len(obscure_mids)),\n",
    "    obscure_mids,\n",
    "])\n",
    "recc_df = movies.loc[obscure_mids].copy()\n",
    "recc_df['l2_predicted_rating'] = preds + offset\n",
    "recc_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#$END_OF_EMB_EXERCISE(82826936984274)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_BELOW%%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`</exercise>`\n",
    "\n",
    "# Scratch space below - please ignore\n",
    "\n",
    "```\n",
    "TODO\n",
    "- (maybe) look at dist. of weights of r12n model vs. prev model\n",
    "- (maybe) load another pretrained model with even stronger r12n. Compare reccs. What's the behaviour in the limit as we increase r12n strength?\n",
    "- Look at reccs for a few other users with fairly clear, distinctive tastes. e.g...\n",
    "    112287 (American Beauty, The Notebook, Mean Girls, The Devil Wears Prada)\n",
    "    69106 (Terminator 2, Toy Story, WALL-E, Days of Thunder, Star Wars, The Matrix)\n",
    "    83421 (The Godfather, The Shawshank Redemption, Casino, Casablanca)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch space / brainstorm\n",
    "\n",
    "Idea: focus on r12n? (In the same way that I was thinking of having ex 1 be walking through adding biases step by step.)\n",
    "\n",
    "Are there other datasets I could have them do factorization on? (Could then be fun to use those learned embeddings for next two exercises)\n",
    "\n",
    "- million song dataset https://www.kaggle.com/c/msdchallenge\n",
    "- goodreads https://www.kaggle.com/zygmunt/goodbooks-10k/home <--- this one looks promising\n",
    "\n",
    "## Recommendations\n",
    "\n",
    "This technique makes it easier to generate recommendations for a particular user. Let's try it. (I guess this is a point where having some understanding of the matrix factorization aspect would be useful.)\n",
    "\n",
    "To make recommendations for user u...\n",
    "- could individually calculate predicted scores for every movie in the dataset, but that'd be pretty tedious.\n",
    "- multiply user vector by weight matrix to get a column vector with predicted scores per movie. Sort.\n",
    "\n",
    "## convergence properties (thought experiment)\n",
    "\n",
    "Compare the loss over time of some DNN models like the ones we trained yesterday vs. some factorization models. Do you notice a difference? Can you think of why this would be?\n",
    "\n",
    "## factorization vs. dnn (thought experiment)\n",
    "\n",
    "We seem to be getting better results with our factorization model. But can you think of situations where you would want to use the dnn model instead?\n",
    "\n",
    "## r12n\n",
    "\n",
    "Look at train vs. val loss for different embedding sizes. How could we prevent overfitting while keeping a larger embedding size? "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "learntools_metadata": {
   "lesson_index": 1,
   "type": "exercise"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise (embeddings)\n",
    "\n",
    "In this exercise you'll answer some questions about embeddings and experiment with a couple variations on the movie rating prediction model we trained in the [tutorial](#$TUTORIAL_URL$).\n",
    "\n",
    "Run the cell below to do some setup. (It will take a minute or two to run, since we're training a couple models for use later in the exercise. While it's running, you can jump ahead and start on part 1.)\n",
    "\n",
    "> *Aside*: Training the model in the tutorial took around a minute per epoch. To make training time more manageable, we'll be working with a sample of 10% of the total dataset (or around 2 million ratings). We've also limited our sample to relatively popular movies (movies with more than 1k ratings in the full dataset). For these reasons, our results in this exercise won't be directly comparable to the error rates reported in the tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup code. Make sure you run this first!\n",
    "\n",
    "import os\n",
    "import random\n",
    "import math\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from matplotlib import pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from learntools.core import binder; binder.bind(globals())\n",
    "from learntools.embeddings.ex1_embedding_layers import *\n",
    "\n",
    "#_RM_\n",
    "input_dir = '../input/movielens_preprocessed'\n",
    "#_UNCOMMENT_\n",
    "#input_dir = '../input'\n",
    "\n",
    "# Load a 10% subset of the full MovieLens data.\n",
    "df = pd.read_csv(os.path.join(input_dir, 'mini_rating.csv'))\n",
    "\n",
    "# Some hyperparameters. (You might want to play with these later)\n",
    "LR = .005 # Learning rate\n",
    "EPOCHS = 8 # Default number of training epochs (i.e. cycles through the training data)\n",
    "hidden_units = (32,4) # Size of our hidden layers\n",
    "\n",
    "def build_and_train_model(movie_embedding_size=8, user_embedding_size=8, verbose=2, epochs=EPOCHS):\n",
    "    tf.set_random_seed(1); np.random.seed(1); random.seed(1) # Set seeds for reproducibility\n",
    "\n",
    "    user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "    movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "    user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                           input_length=1, name='user_embedding')(user_id_input)\n",
    "    movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                            input_length=1, name='movie_embedding')(movie_id_input)\n",
    "    concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "    out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "    # Add one or more hidden layers\n",
    "    for n_hidden in hidden_units:\n",
    "        out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "    # A single output: our predicted rating\n",
    "    out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "    model = keras.Model(\n",
    "        inputs = [user_id_input, movie_id_input],\n",
    "        outputs = out,\n",
    "    )\n",
    "    model.compile(\n",
    "        tf.train.AdamOptimizer(LR),\n",
    "        loss='MSE',\n",
    "        metrics=['MAE'],\n",
    "    )\n",
    "    history = model.fit(\n",
    "        [df.userId, df.movieId],\n",
    "        df.y,\n",
    "        batch_size=5 * 10**3,\n",
    "        epochs=epochs,\n",
    "        verbose=verbose,\n",
    "        validation_split=.05,\n",
    "    )\n",
    "    return history\n",
    "\n",
    "# Train two models with different embedding sizes and save the training statistics.\n",
    "# We'll be using this later in the exercise.\n",
    "history_8 = build_and_train_model(verbose=0)\n",
    "history_32 = build_and_train_model(32, 32, verbose=0)\n",
    "\n",
    "print(\"Setup complete!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: When to use embeddings?\n",
    "\n",
    "You're a data scientist for the hot new music streaming service, Tidify All Accessâ„¢ with the task of building a machine learning model to generate suggested songs to show users. You have lots of historical data about songs that users have listened to in the past. The dataset of 500 million streams includes the following variables:\n",
    "\n",
    "| Variable name | Description                                       | Number of unique values | Example values\n",
    "|---------------|---------------------------------------------------|------|----------------------------------\n",
    "| stream_id     | Unique id for this streaming event (i.e. per row) | 500m | 480744481269, 228441807745, 182969356277, ...\n",
    "| user_id       | Unique id for this user                           | 5m   | 3592022173, 2596402742, 3506743568, ...\n",
    "| song_id       | Unique id for this song                           | 10m  | 3150630225,  590655137, 3617674674, ...\n",
    "| timestamp     | When did the song start playing?                  | 80m  | 10/08/2017 01:20:44 PM, 04/22/2017 01:58:59 PM, ...\n",
    "| artist_id     | Unique id for the recording artist of this song   | 1m   | 122168143,  52958907, 803608525, ...\n",
    "| song_duration | How long is the song (in seconds)?                | 450  | 257, 155, 212...\n",
    "| explicit      | Is this song flagged as having adult language?    | 2    | True, False\n",
    "| user_country  | Where is this user located? (3-letter ISO country code) | 150  | CAN, KOR, CHE...\n",
    "\n",
    "If you wanted to train a neural net on streaming data, which of the variables above would you include and use use an embedding layer for? Use the variable `embedding_variables` below to give your answer.\n",
    "\n",
    "(You can assume that, if necessary, string variables can be converted to unique numerical identifiers as a preprocessing step.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding_variables should contain all the variables you would use an embedding layer for\n",
    "# For your convenience, we've initialized it with all variables in the dataset, so you can \n",
    "# just delete or comment out the variables you want to exclude.\n",
    "embedding_variables = {\n",
    "    'stream_id',\n",
    "    'user_id',\n",
    "    'song_id',\n",
    "    'timestamp',\n",
    "    'artist_id',\n",
    "    'song_duration',\n",
    "    'explicit',\n",
    "    'user_country',\n",
    "}\n",
    "part1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Incorrect\n",
    "embedding_variables = {\n",
    "    'stream_id',\n",
    "}\n",
    "part1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Correct 1\n",
    "embedding_variables = {\n",
    "    'user_id',\n",
    "    'song_id',\n",
    "    'artist_id',\n",
    "}\n",
    "part1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Correct 2 (optional extra val)\n",
    "embedding_variables = {\n",
    "    'user_id',\n",
    "    'song_id',\n",
    "    'artist_id',\n",
    "    'user_country',\n",
    "}\n",
    "part1.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part1.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Choosing embedding sizes\n",
    "\n",
    "In the [tutorial](#$TUTORIAL_URL$), we (somewhat arbitrarily) set `output_dim=8` when creating our movie and user embedding layers. Run the code cell below to see a visualization of our training and validation error over 8 epochs of training, again using 8-dimensional embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_FS = (15, 5)\n",
    "def plot_history(histories, keys=('mean_absolute_error',), train=True, figsize=history_FS):\n",
    "    if isinstance(histories, tf.keras.callbacks.History):\n",
    "        histories = [ ('', histories) ]\n",
    "    for key in keys:\n",
    "        plt.figure(figsize=history_FS)\n",
    "        for name, history in histories:\n",
    "            val = plt.plot(history.epoch, history.history['val_'+key],\n",
    "                           '--', label=str(name).title()+' Val')\n",
    "            if train:\n",
    "                plt.plot(history.epoch, history.history[key], color=val[0].get_color(), alpha=.5,\n",
    "                         label=str(name).title()+' Train')\n",
    "\n",
    "        plt.xlabel('Epochs')\n",
    "        plt.ylabel(key.replace('_',' ').title())\n",
    "        plt.legend()\n",
    "        plt.title(key)\n",
    "\n",
    "        plt.xlim([0,max(max(history.epoch) for (_, history) in histories)])\n",
    "\n",
    "plot_history([ \n",
    "    ('base model', history_8),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the start of the notebook we also trained a model with 64-dimensional movie and user embeddings. How do you expect the results to compare? Make a prediction, then run the cell below to see."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([ \n",
    "    ('8-d embeddings', history_8),\n",
    "    ('32-d embeddings', history_32),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're feeling experimental, feel free to try some other configurations. So far we've varied movie and user embedding size in lock step, but there's no reason they have to be the same. Do you have an intuition about whether one or the other should be bigger?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: shrinking movie embeddings and growing user embeddings\n",
    "#history_biguser_smallmovie = build_and_train_model(movie_embedding_size=4, user_embedding_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "history_biguser_smallmovie = build_and_train_model(movie_embedding_size=4, user_embedding_size=16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "plot_history([ \n",
    "    ('8-d embeddings', history_8),\n",
    "    ('32-d embeddings', history_32),\n",
    "    ('m4u16', history_biguser_smallmovie),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you're ready, uncomment the cell below for some explanation of what's going on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part2.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Adding biases\n",
    "\n",
    "In this final section, you'll implement a modification to our model's architecture: per-movie biases.\n",
    "\n",
    "In ML-speak, a **bias** is just a number that gets added to a node's output value. For each movie, we'll learn a single number that we'll add to the output of what was previously our final node. Here's what that looks like:\n",
    "\n",
    "<img src=\"https://docs.google.com/a/google.com/drawings/d/e/2PACX-1vSmf5H7Rcr771flhidl7Wf31OXiZTUgNH2qzoVc-2dtH6Cf9XmSF3xQcY7m1RwCRBu2_lE-dH5Vb6ny/pub?w=1050&h=594\" />\n",
    "\n",
    "### Why?\n",
    "\n",
    "Do you think this will improve the accuracy of our predictions? Why or why not?\n",
    "\n",
    "Think about it, then uncomment the line below to see an explanation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.a.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you have an idea about what the bias values will look like? Are there certain movies you expect will have high or low biases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.b.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Coding up biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below is the code we used to train our embedding model in the tutorial. Modify it (where indicated by the comments) to add per-movie biases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_embedding_size = movie_embedding_size = 8\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "# Add one or more hidden layers\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "# A single output: our predicted rating (before adding bias)\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "################################################################################\n",
    "############################# YOUR CODE GOES HERE! #############################\n",
    "# TODO: you need to create the variable movie_bias. Its value should be the output of calling a layer.\n",
    "# I recommend giving the layer that holds your biases a distinctive name (this will help in an upcoming question)\n",
    "#movie_bias = \n",
    "################################################################################\n",
    "out = keras.layers.Add()([out, movie_bias])\n",
    "\n",
    "model_bias = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "model_bias.compile(\n",
    "    tf.train.AdamOptimizer(LR),\n",
    "    loss='MSE',\n",
    "    metrics=['MAE'],\n",
    ")\n",
    "model_bias.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No idea where to start? Don't panic! Uncomment and run the line below for some hints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.c.hint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.c.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Bad solution (no bias adding)\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "# Add one or more hidden layers\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "# A single output: our predicted rating\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "bias_embedded = keras.layers.Embedding(df.movieId.max()+1, 1, input_length=1, name='bias',\n",
    "                                      )(movie_id_input)\n",
    "movie_bias = keras.layers.Flatten()(bias_embedded)\n",
    "#out = keras.layers.Add()([out, movie_bias])\n",
    "\n",
    "model_bias = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "model_bias.compile(\n",
    "    tf.train.AdamOptimizer(LR),\n",
    "    loss='MSE',\n",
    "    metrics=['MAE'],\n",
    ")\n",
    "part3.c.check()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Canonical correct solution\n",
    "user_embedding_size = movie_embedding_size = 8\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "# Add one or more hidden layers\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "# A single output: our predicted rating (before adding bias)\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "################################################################################\n",
    "############################# YOUR CODE GOES HERE! #############################\n",
    "# TODO: you need to create the variable movie_bias. Its value should be the output of calling a layer.\n",
    "# I recommend giving the layer that holds your biases a distinctive name (this will help in an upcoming question)\n",
    "bias_embedded = keras.layers.Embedding(df.movieId.max()+1, 1, input_length=1, name='bias',\n",
    "                                      )(movie_id_input)\n",
    "movie_bias = keras.layers.Flatten()(bias_embedded)\n",
    "################################################################################\n",
    "out = keras.layers.Add()([out, movie_bias])\n",
    "\n",
    "model_bias = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "model_bias.compile(\n",
    "    tf.train.AdamOptimizer(LR),\n",
    "    loss='MSE',\n",
    "    metrics=['MAE'],\n",
    ")\n",
    "part3.c.check()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "\n",
    "Run the code below to train the model you built in the previous step. (If you get an error when trying to fit your model, you may have made a mistake in your bias implementation. See `part3.c.solution()` for a working implementation.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_bias = model_bias.fit(\n",
    "    [df.userId, df.movieId],\n",
    "    df.y,\n",
    "    batch_size=5 * 10**3,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=2,\n",
    "    validation_split=.05,\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does it compare to the results we got from the model without biases? Run the code cell below to compare their loss over the course of training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history([ \n",
    "    ('no_bias', history_8),\n",
    "    ('bias', history_bias),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How did adding biases affect our results?\n",
    "\n",
    "Averaged over many runs, biases seem to help a bit, but there's enough variance between runs (as a result of different random initializations), that you might be seeing better or worse results. If you're seeing a *big* difference (more than, say, +-.02 in the final loss) in either direction, something may have gone wrong.\n",
    "\n",
    "So biases weren't the huge win we might have hoped for, but it still seems worth testing our hypothesis about how bias values will be distributed among movies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspecting learned bias values\n",
    "\n",
    "Let's take a look at the bias values our model has learned. Fill in the missing code in the cell below to load an array of bias values - `b` should be an array with one number for each movie in our training set.\n",
    "\n",
    "**Hint:** you may want to check out the docs for [`keras.Model.get_layer`](https://www.tensorflow.org/api_docs/python/tf/keras/Model#get_layer). This will be easier if you gave your `Embedding` bias layer a distinctive name in part 2. If you didn't, it may help to look at the output of `model_bias.summary()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bias_layer = None\n",
    "\n",
    "part3.d.check()\n",
    "\n",
    "(b,) = bias_layer.get_weights()\n",
    "print(\"Loaded biases with shape {}\".format(b.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#_COMMENT_IF(PROD)_\n",
    "part3.d.solution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Bad solution (wrong layer)\n",
    "bias_layer = model_bias.get_layer(index=10)\n",
    "part3.d.check()\n",
    "\n",
    "(b,) = bias_layer.get_weights()\n",
    "print(\"Loaded biases with shape {}\".format(b.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Canonical correct solution\n",
    "bias_layer = model_bias.get_layer('bias')\n",
    "part3.d.check()\n",
    "\n",
    "(b,) = bias_layer.get_weights()\n",
    "print(\"Loaded biases with shape {}\".format(b.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once you've successfully set the value of `bias_layer`, run the cell below which loads a dataframe containing movie metadata and adds the biases found in the previous step as a column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movies = pd.read_csv(os.path.join(input_dir, 'movie.csv'), index_col=0, \n",
    "                     usecols=['movieId', 'title', 'genres', 'year'])\n",
    "ntrain = math.floor(len(df) * .95)\n",
    "df_train = df.head(ntrain)\n",
    "\n",
    "# Mapping from original movie ids to canonical ones\n",
    "mids = movieId_to_canon = df.groupby('movieId')['movieId_orig'].first()\n",
    "# Add bias column\n",
    "movies.loc[mids.values, 'bias'] = b\n",
    "# Add columns for number of ratings and average rating\n",
    "g = df_train.groupby('movieId_orig')\n",
    "movies.loc[mids.values, 'n_ratings'] = g.size()\n",
    "movies.loc[mids.values, 'mean_rating'] = g['rating'].mean()\n",
    "\n",
    "movies.dropna(inplace=True)\n",
    "\n",
    "movies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which movies have the lowest and highest learned biases? Run the cell below to find out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display\n",
    "n = 10\n",
    "display(\n",
    "    \"Largest biases...\",\n",
    "    movies.sort_values(by='bias', ascending=False).head(n),\n",
    "    \"Smallest biases...\",\n",
    "    movies.sort_values(by='bias').head(n),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the cell below to generate a scatter plot of movies' average ratings against the biases learned for those movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 1000\n",
    "mini = movies.sample(n, random_state=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "ax.scatter(mini['mean_rating'], mini['bias'], alpha=.4)\n",
    "ax.set_xlabel('Mean rating')\n",
    "ax.set_ylabel('Bias');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_IF(PROD)%%\n",
    "# Experiment: Draw line of expected fit\n",
    "n = 1000\n",
    "mini = movies.sample(n, random_state=1)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(13, 7))\n",
    "ax.scatter(mini['mean_rating'], mini['bias'], alpha=.4)\n",
    "\n",
    "row = df.iloc[0]\n",
    "global_mean_rating = row.rating - row.y\n",
    "lw = 1\n",
    "ax.axhline(0, ls='--', color='grey', lw=lw)\n",
    "ax.axvline(global_mean_rating, ls='--', label='Overall mean rating', color='indigo', lw=lw)\n",
    "# Draw line of expected fit (if biases were to match data means)\n",
    "pt = (global_mean_rating, 0)\n",
    "x0, y0 = 0.5, (0.5 - global_mean_rating)\n",
    "x1, y1 = 5, (5 - global_mean_rating)\n",
    "ylim0 = ax.get_ylim()\n",
    "ax.plot([x0, x1], [y0, y1], linestyle='-', lw=3, color='lime', alpha=.5)\n",
    "ax.set_ylim(ylim0)\n",
    "\n",
    "ax.set_xlabel('Mean rating')\n",
    "ax.set_ylabel('Bias');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Considering this plot and the list of our highest and lowest bias movies, do our model's learned biases agree with what you expected?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM%%\n",
    "# (Discussion in solution text doesn't really make sense anymore now that we're thresholding obscure movies\n",
    "# for this exercise.)\n",
    "#_COMMENT_IF(PROD)_\n",
    "part3.e.solution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#$END_OF_EMB_EXERCISE(82826270084256)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%%RM_BELOW%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "########### SCRATCH BELOW ##########\n",
    "####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "That's the end of this exercise. How'd it go? If you have any questions, be sure to post them on the [forums](https://www.kaggle.com/learn-forum).\n",
    "\n",
    "*This course is still in beta*, so I'd love to get your feedback. If you have a moment to [fill out a super-short survey about this exercise]({}), I'd greatly appreciate it.\n",
    "\n",
    "# Keep going\n",
    "\n",
    "When you're ready to continue, [click here]({}) to continue on to the next tutorial on {}."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Your Turn\n",
    "\n",
    "Head over to [the Exercises notebook]({}) to get some hands-on practice working with {}.\n",
    "\n",
    "---\n",
    "\n",
    "*P.S.*: This course is still in Beta, so I'd love to get your feedback. If you have a moment to [fill out a super-short survey about this lesson](TODO), I'd greatly appreciate it. You can also leave any feedback or questions in the comments below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: User biases\n",
    "\n",
    "It turns out that users have a much less spiky distribution in the dataset (by construction - the MovieLens dataset only included users who gave at least 20 ratings). For this reason, adding per-*user* biases tends to have a more consistently positive effect on accuracy.\n",
    "\n",
    "Run the cell below to train a model with per-user biases, and compare the results with our earlier models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.set_random_seed(1); np.random.seed(1); random.seed(1)\n",
    "\n",
    "user_id_input = keras.Input(shape=(1,), name='user_id')\n",
    "movie_id_input = keras.Input(shape=(1,), name='movie_id')\n",
    "user_embedded = keras.layers.Embedding(df.userId.max()+1, user_embedding_size, \n",
    "                                       input_length=1, name='user_embedding')(user_id_input)\n",
    "movie_embedded = keras.layers.Embedding(df.movieId.max()+1, movie_embedding_size, \n",
    "                                        input_length=1, name='movie_embedding')(movie_id_input)\n",
    "concatenated = keras.layers.Concatenate()([user_embedded, movie_embedded])\n",
    "out = keras.layers.Flatten()(concatenated)\n",
    "\n",
    "for n_hidden in hidden_units:\n",
    "    out = keras.layers.Dense(n_hidden, activation='relu')(out)\n",
    "\n",
    "out = keras.layers.Dense(1, activation='linear', name='prediction')(out)\n",
    "\n",
    "bias_embedded = keras.layers.Embedding(df.userId.max()+1, 1, input_length=1, name='bias',\n",
    "                                       # It's typical to initialize embeddings to zero (rather than random noise)\n",
    "                                       embeddings_initializer='zeros',\n",
    "                                      )(user_id_input)\n",
    "user_bias = keras.layers.Flatten()(bias_embedded)\n",
    "out = keras.layers.Add()([out, user_bias])\n",
    "\n",
    "model = keras.Model(\n",
    "    inputs = [user_id_input, movie_id_input],\n",
    "    outputs = out,\n",
    ")\n",
    "model.compile(\n",
    "    tf.train.AdamOptimizer(LR),\n",
    "    loss='MSE',\n",
    "    metrics=['MAE'],\n",
    ")\n",
    "\n",
    "history_ubias = model.fit(\n",
    "    [df.userId, df.movieId],\n",
    "    df.y,\n",
    "    batch_size=5 * 10**3,\n",
    "    epochs=EPOCHS,\n",
    "    verbose=2,\n",
    "    validation_split=.05,\n",
    ");\n",
    "plot_history([ \n",
    "    ('no_bias', history),\n",
    "    ('movie bias', history_bias),\n",
    "    ('user bias', history_ubias),\n",
    "]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scratch space. Other exercise ideas.\n",
    "\n",
    "## Experimenting with embedding sizes\n",
    "\n",
    "In the lesson, we (fairly arbitrarily) chose an embedding size of 8 for both movies and users. The size of embeddings is a hyperparameter we can experiment with to improve our results. Larger embeddings give the model more capacity, meaning they can potentially learn more detailed representations. On the other hand, it increases the model's ability to 'memorize' details of the training data that don't generalize well to unseen data.\n",
    "\n",
    "Note that the size of our movie embeddings and our user embeddings need not be the same. Do you have a hunch about whether one should have bigger embeddings than the other?\n",
    "\n",
    "Try experimenting with a few different embedding sizes. Can you improve our loss on the validation data? How do different embedding sizes affect the training loss?\n",
    "\n",
    "Hint: [this TensorFlow tutorial](https://www.tensorflow.org/guide/feature_columns) gives a general rule of thumb for setting the embedding size of a categorical variable: the 4th root of the number of possible values.\n",
    "\n",
    "## . adding auxiliary features\n",
    "\n",
    "#### DB: I really like auxiliary information.  I think it makes the distinction with embeddings more concrete.  I've also seen applications where it's helped a lot, so it'd be good for users to know. I'd try to avoid adding issues of padding here, since we're still just trying to intro the concepts in embeddings.\n",
    "(Note: question of how to incorporate genre is actually a great prelude to the stuff talked about in the cooking problem. Movies have varying numbers of genre tags, so need some way to deal w/ variable-sized bags of categorical variables. Padding, combining embeddings. Just like cooking problem.)\n",
    "\n",
    "## . Movie embeddings only (thought experiment)\n",
    "\n",
    "Mary the movie buff has decided to train a variant of our model. She thinks that movies have a lot of interesting dimensions of variation, and are highly useful for our prediction task, so she allocates a lot more capacity for them - she increases the embedding size from 32 to 128.\n",
    "\n",
    "On the other hand, she thinks that differences between users are too small to pick out from the noise, so she doesn't train any user embeddings at all.\n",
    "\n",
    "There's a fundamental issue with her model. Do you see what it is? How would you explain it to her?\n",
    "\n",
    "*Solution:* Regardless of how big she makes the embeddings, or how many hidden layers she adds, as long as her model only takes movie ids as inputs, it's equivalent to a simple linear model that learns one rating per movie id. Therefore, she might as well just train a simple linear regression model directly - or even calculate the optimal values per movie directly, which is just its mean rating. \n",
    "\n",
    "## . clamping output range (spicy)\n",
    "\n",
    "#### DB: I'd prefer questions that are conceptually about embeddings. This seems like an interesting question, but it's not clear to me that it's topical to this course.\n",
    "We know that ratings can only ever take on the values `{0.5, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5}`.\n",
    "\n",
    "Can we help our model out by forcing its output into that range? Should we?\n",
    "\n",
    "### pt. 1\n",
    "\n",
    "If our model makes a prediction less than 0.5, we're guaranteed to get a better result if we bump it up to 0.5. Similarly if it makes predictions above 5.0. But is it doing this? \n",
    "\n",
    "Calculate some predictions on the training data and on the validation data. Do you see any predictions outside the range `[0.5, 5.0]`? \n",
    "\n",
    "### pt2. \n",
    "\n",
    "Modify the code below to prevent the model from making predictions outside the allowable range of outputs, then try training it. How do results compare to the original model?\n",
    "\n",
    "Solution: Change the final layer's activation from linear to a function with a fixed range (sigmoid or tanh). From there, you'll need to use a Lambda Layer to scale the output range to `[0.5, 5]` by some addition and multiplication. Alternatively: scale the `y` values ahead of time to the range `[0, 1]` (or `[-1, 1]`, if using tanh). But note: now the reported MSE/MAE is not comparable to our other models.\n",
    "\n",
    "\n",
    "## other\n",
    "\n",
    "- r12n. encouraging sparsity\n",
    "- prediction debugging\n",
    "- thought experiment: regression v multiclass classification\n",
    "\n",
    "#### DB Ideas: \n",
    "- Compare the errors for users who have made different number of recommendations\n",
    "- ThoughtQuestion with some example use cases. Ask where embeddings would do well and where one-hot encoding would do well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model introspection sandbox\n",
    "model = model_bias\n",
    "final = model.layers[-1]\n",
    "assert isinstance(final, keras.layers.Add)\n",
    "cfg = model.get_config()\n",
    "print(cfg.keys())\n",
    "print(cfg['name'])\n",
    "f = cfg['layers'][-1]\n",
    "print(f)\n",
    "f['inbound_nodes'][0]\n",
    "#final.outbound_nodes\n",
    "#final.inbound_nodes[0]\n",
    "[layer for layer in cfg['layers'] if layer['name'] == 'flatten_1']\n",
    "blc = [layer for layer in cfg['layers'] if layer['name'] == 'bias'][0]\n",
    "print()\n",
    "print(blc.keys(),\n",
    "    blc['config'],\n",
    "      sep='\\n'\n",
    "     )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "learntools_metadata": {
   "lesson_index": 0,
   "type": "exercise"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

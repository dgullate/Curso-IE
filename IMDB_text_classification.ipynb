{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "IMDB_text_classification.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dgullate/Curso-IE/blob/master/IMDB_text_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OKGzj9zjc9gT",
        "colab_type": "text"
      },
      "source": [
        "## Loading and exploring the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VoOH7KdoleQ2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import textblob\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "import re\n",
        "import pickle\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "from sklearn import preprocessing, model_selection, metrics\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split,cross_val_score\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import f1_score, make_scorer,roc_curve, roc_auc_score\n",
        "\n",
        "import os"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlVfZ6splfJW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_path='https://github.com/dgullate/Curso-IE/blob/master/Week_4/IMDB/data/labeledTrainData.tsv?raw=true'\n",
        "test_data_path='https://github.com/dgullate/Curso-IE/blob/master/Week_4/IMDB/data/testData.tsv?raw=true'\n",
        "unlabeled_data_path='https://github.com/dgullate/Curso-IE/blob/master/Week_4/IMDB/data/unlabeledTrainData.tsv?raw=true'\n",
        "train = pd.read_csv(train_data_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "raw_train_data_unlabeled = pd.read_csv(unlabeled_data_path, header=0, delimiter=\"\\t\", quoting=3)\n",
        "raw_test_data = pd.read_csv(test_data_path, header=0, delimiter=\"\\t\", quoting=3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HNT4A8OLnEF3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train.head())\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v_pOe1PvnNDc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KTkR8cm6rTmP",
        "colab_type": "text"
      },
      "source": [
        "## Data Cleaning and Text Preprocessing\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8g5LWH1rcP4U",
        "colab_type": "text"
      },
      "source": [
        "### Removing HTML Markup: The BeautifulSoup Package"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6dTwDivnhdG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Initialize the BeautifulSoup object on a single movie review     \n",
        "example1 = BeautifulSoup(train[\"review\"][0])  \n",
        "\n",
        "# Print the raw review and then the output of get_text(), for \n",
        "# comparison\n",
        "print(train[\"review\"][0])\n",
        "print (example1.get_text())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hvXqCg83qKnK",
        "colab_type": "text"
      },
      "source": [
        "Calling ```get_text()``` gives you the text of the review, without tags or markup. If you browse the BeautifulSoup documentation, you'll see that it's a very powerful library - more powerful than we need for this dataset. However, it is not considered a reliable practice to remove markup using regular expressions, so even for an application as simple as this, it's usually best to use a package like BeautifulSoup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a6aMJPHwn4oA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bi-HUmf2rlCP",
        "colab_type": "text"
      },
      "source": [
        "### Dealing with Punctuation, Numbers and Stopwords: NLTK and regular expressions\n",
        "When considering how to clean the text, we should think about the data problem we are trying to solve. For many problems, it makes sense to remove punctuation. On the other hand, in this case, we are tackling a sentiment analysis problem, and it is possible that \"!!!\" or \":-(\" could carry sentiment, and should be treated as words. In this tutorial, for simplicity, we remove the punctuation altogether, but it is something you can play with on your own.\n",
        "\n",
        "Similarly, in this tutorial we will remove numbers, but there are other ways of dealing with them that make just as much sense. For example, we could treat them as words, or replace them all with a placeholder string such as \"NUM\"."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR9RSalOrmry",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "# Use regular expressions to do a find-and-replace\n",
        "letters_only = re.sub(\"[^a-zA-Z]\",           # The pattern to search for\n",
        "                      \" \",                   # The pattern to replace it with\n",
        "                      example1.get_text() )  # The text to search\n",
        "print(letters_only)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MMRUHFInrzNs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0YPaTCxDsCrV",
        "colab_type": "text"
      },
      "source": [
        "A full overview of regular expressions is beyond the scope of this tutorial, but for now it is sufficient to know that [] indicates group membership and ^ means \"not\". In other words, the re.sub() statement above says, \"Find anything that is NOT a lowercase letter (a-z) or an upper case letter (A-Z), and replace it with a space.\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MEsrGrZVsDaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "lower_case = letters_only.lower()        # Convert to lower case\n",
        "words = lower_case.split()               # Split into words"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1n1T-GH_sWxh",
        "colab_type": "text"
      },
      "source": [
        "Finally, we need to decide how to deal with frequently occurring words that don't carry much meaning. Such words are called \"stop words\"; in English they include words such as \"a\", \"and\", \"is\", and \"the\". Conveniently, there are Python packages that come with stop word lists built in. Let's import a stop word list from the Python Natural Language Toolkit (NLTK)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hATQS6iVsL8v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords') # Download text data sets, including stop words\n",
        "from nltk.corpus import stopwords # Import the stop word list\n",
        "print(stopwords.words(\"english\") )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mKZwjhuZs5r4",
        "colab_type": "text"
      },
      "source": [
        "This will allow you to view the list of English-language stop words. To remove stop words from our movie review, do:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w_-8Dax9sh-y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Remove stop words from \"words\"\n",
        "words = [w for w in words if not w in stopwords.words(\"english\")]\n",
        "print(words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFOZVuvrtHAJ",
        "colab_type": "text"
      },
      "source": [
        "There are many other things we could do to the data - For example, Porter Stemming and Lemmatizing (both available in NLTK) would allow us to treat \"messages\", \"message\", and \"messaging\" as the same word, which could certainly be useful. However, for simplicity, we will stop here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sw-Ag0OHtUyD",
        "colab_type": "text"
      },
      "source": [
        "### Putting it all together\n",
        "Now we have code to clean one review - but we need to clean 25,000 training reviews! To make our code reusable, let's create a function that can be called many times:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rs64CZcetIBD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def review_to_words( raw_review ):\n",
        "    # Function to convert a raw review to a string of words\n",
        "    # The input is a single string (a raw movie review), and \n",
        "    # the output is a single string (a preprocessed movie review)\n",
        "  \n",
        "    # 1. Remove HTML\n",
        "    review_text = BeautifulSoup(raw_review).get_text() \n",
        "    #\n",
        "    # 2. Remove non-letters        \n",
        "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", review_text) \n",
        "    #\n",
        "    # 3. Convert to lower case, split into individual words\n",
        "    words = letters_only.lower().split()                             \n",
        "    #\n",
        "    # 4. In Python, searching a set is much faster than searching\n",
        "    #   a list, so convert the stop words to a set\n",
        "    stops = set(stopwords.words(\"english\"))                  \n",
        "    # \n",
        "    # 5. Remove stop words\n",
        "    meaningful_words = [w for w in words if not w in stops]   \n",
        "    #\n",
        "    # 6. Join the words back into one string separated by space, \n",
        "    # and return the result.\n",
        "    return( \" \".join( meaningful_words ))   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_p8DqRl7ujX6",
        "colab_type": "text"
      },
      "source": [
        "Two elements here are new: First, we converted the stop word list to a different data type, a set. This is for speed; since we'll be calling this function tens of thousands of times, it needs to be fast, and searching sets in Python is much faster than searching lists.\n",
        "\n",
        "Second, we joined the words back into one paragraph. This is to make the output easier to use in our Bag of Words, below. After defining the above function, if you call the function for a single review:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBvEA4EauenN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "clean_review = review_to_words( train[\"review\"][0] )\n",
        "print(clean_review)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FLX6e3lEu7Od",
        "colab_type": "text"
      },
      "source": [
        "it should give you exactly the same output as all of the individual steps we did in preceding tutorial sections. Now let's loop through and clean all of the training set at once (this might take a few minutes depending on your computer):"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rFR2r3HuqC_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Get the number of reviews based on the dataframe column size\n",
        "num_reviews = train[\"review\"].size\n",
        "\n",
        "# Initialize an empty list to hold the clean reviews\n",
        "clean_train_reviews = []\n",
        "\n",
        "# Loop over each review; create an index i that goes from 0 to the length\n",
        "# of the movie review list \n",
        "for i in range( 0, num_reviews ):\n",
        "    # Call our function for each one, and add the result to the list of\n",
        "    # clean reviews\n",
        "    clean_train_reviews.append( review_to_words( train[\"review\"][i] ) )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBVM5OKqvFAa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train[\"clean_review\"]=train[\"review\"].map(lambda x:review_to_words(x))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YBfH1fc_vRpb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_hNJZIKDwdrB",
        "colab_type": "text"
      },
      "source": [
        "Now we have already prepared our training data for the next step of the process: feature extraction."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "woFVKlAOwmQC",
        "colab_type": "text"
      },
      "source": [
        "## Creating Features \n",
        "Now that we have our training reviews tidied up, how do we convert them to some kind of numeric representation for machine learning? "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OytNJsTPckzH",
        "colab_type": "text"
      },
      "source": [
        "### Bag of Words features (with scikit-learn)\n",
        "One common approach is called a Bag of Words. The Bag of Words model learns a vocabulary from all of the documents, then models each document by counting the number of times each word appears. For example, consider the following two sentences:\n",
        "\n",
        "Sentence 1: \"The cat sat on the hat\"\n",
        "\n",
        "Sentence 2: \"The dog ate the cat and the hat\"\n",
        "\n",
        "From these two sentences, our vocabulary is as follows:\n",
        "\n",
        "{ the, cat, sat, on, hat, dog, ate, and }\n",
        "\n",
        "To get our bags of words, we count the number of times each word occurs in each sentence. In Sentence 1, \"the\" appears twice, and \"cat\", \"sat\", \"on\", and \"hat\" each appear once, so the feature vector for Sentence 1 is:\n",
        "\n",
        "{ the, cat, sat, on, hat, dog, ate, and }\n",
        "\n",
        "Sentence 1: { 2, 1, 1, 1, 1, 0, 0, 0 }\n",
        "\n",
        "Similarly, the features for Sentence 2 are: { 3, 1, 0, 0, 1, 1, 1, 1}\n",
        "\n",
        "In the IMDB data, we have a very large number of reviews, which will give us a large vocabulary. To limit the size of the feature vectors, we should choose some maximum vocabulary size. Below, we use the 5000 most frequent words (remembering that stop words have already been removed).\n",
        "\n",
        "We'll be using the feature_extraction module from scikit-learn to create bag-of-words features."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_5xz9Kq7v0Wo",
        "colab_type": "code",
        "outputId": "b2c1d294-dd05-4c28-8649-0e2dd84cfaf3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "print(\"Creating the bag of words...\\n\")\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "# Initialize the \"CountVectorizer\" object, which is scikit-learn's\n",
        "# bag of words tool.  \n",
        "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
        "                             tokenizer = None,    \\\n",
        "                             preprocessor = None, \\\n",
        "                             stop_words = None,   \\\n",
        "                             max_features = 5000) \n",
        "\n",
        "# fit_transform() does two functions: First, it fits the model\n",
        "# and learns the vocabulary; second, it transforms our training data\n",
        "# into feature vectors. The input to fit_transform should be a list of \n",
        "# strings.\n",
        "train_data_features = vectorizer.fit_transform(clean_train_reviews)\n",
        "\n",
        "# Numpy arrays are easy to work with, so convert the result to an \n",
        "# array\n",
        "train_data_features = train_data_features.toarray()"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Creating the bag of words...\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "46mermvTw_XX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_data_features[2].sum()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA5gWKDmOTlx",
        "colab_type": "text"
      },
      "source": [
        "To see what the training data array now looks like, do:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UxqsBGvxIF6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(train_data_features.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L-2YDqAdOeEH",
        "colab_type": "text"
      },
      "source": [
        "It has 25,000 rows and 5,000 features (one for each vocabulary word).\n",
        "\n",
        "Note that CountVectorizer comes with its own options to automatically do preprocessing, tokenization, and stop word removal -- for each of these, instead of specifying \"None\", we could have used a built-in method or specified our own function to use.  See the function documentation for more details. However, we wanted to write our own function for data cleaning in this tutorial to show you how it's done step by step.\n",
        "\n",
        "Now that the Bag of Words model is trained, let's look at the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uMqZWjoyOZeR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a look at the words in the vocabulary\n",
        "vocab = vectorizer.get_feature_names()\n",
        "print(len(vocab))\n",
        "print(vocab)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xjb2ylCnOwC3",
        "colab_type": "text"
      },
      "source": [
        "We can also print the counts of each word in \n",
        "the vocabulary:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zS7kfLjVOmcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Sum up the counts of each vocabulary word\n",
        "dist = np.sum(train_data_features, axis=0)\n",
        "\n",
        "# For each, print the vocabulary word and the number of times it \n",
        "# appears in the training set\n",
        "d={}\n",
        "for tag, count in zip(vocab, dist):\n",
        "    d[tag]=count\n",
        "sorted_d = sorted(d.items(), key=lambda x: x[1],reverse=True)\n",
        "sorted_d[:20]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-W1WgObS2oM",
        "colab_type": "text"
      },
      "source": [
        "## Training classifiers after feature extraction\n",
        "At this point, we have numeric training features from the Bag of Words and the original sentiment labels for each feature vector, so let's do some supervised learning! We can try any classifier of our choice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kQaY8NMlS4Vc",
        "colab_type": "text"
      },
      "source": [
        "Splitting the training test into train and validation sets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "66Q_XndiS9HN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_val, y_train, y_val = train_test_split(train_data_features, train[\"sentiment\"], test_size=0.25, random_state=42)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1eu4_DawogE8",
        "colab_type": "text"
      },
      "source": [
        "Each line in the X_train set is a vector with 5000 integer entries, the counts of each word in the vcabulary in that text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnPSuyIYoVWQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mLNuGmG0VoAU",
        "colab_type": "text"
      },
      "source": [
        "### Logistic Regression\n",
        "\n",
        "We might try as a bsaline with the simplest linear model for classification: just a logistic regression."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FPUfKJlV2St",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "clf=LogisticRegression()\n",
        "clf.fit(X_train,y_train)\n",
        "print('Accuracy on the training set: ',clf.score(X_train,y_train))\n",
        "print('Accuracy on the validation set: ',clf.score(X_val,y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WeWZkzb2WedH",
        "colab_type": "text"
      },
      "source": [
        "The score on the trianing set is much better than on the validation, which is clearly a symptom of **overfitting**.\n",
        "In order to mitigate this, let us include some regularization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8wLMQEGLWt7g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "parameters = {'C':[0.01,0.05,0.1],'penalty':['l1','l2']}\n",
        "clf=LogisticRegression( )\n",
        "grid_clf = GridSearchCV(clf, parameters, cv=3)\n",
        "grid_clf.fit(X_train,y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jD_a-8FYHqM",
        "colab_type": "text"
      },
      "source": [
        "Now we can evaluate the best model and find the best parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uUoqZ1GSYLob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Accuracy on the training set: ',grid_clf.score(X_train,y_train))\n",
        "print('Accuracy on the validation set: ',grid_clf.score(X_val,y_val))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hwp9EZepYWGj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "grid_clf.best_estimator_"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gnRc-ob1aen1",
        "colab_type": "text"
      },
      "source": [
        "We get a result of 88% on the validation set, which is quite alright."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KIrrSYwraelE",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbHjbxZKQHu0",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "###  Random Forest\n",
        "\n",
        "The Random Forest algorithm is included in scikit-learn (Random Forest uses many tree-based classifiers to make predictions, hence the \"forest\"). Below, we set the number of trees to 100 as a reasonable default value. More trees may (or may not) perform better, but will certainly take longer to run. Likewise, the more features you include for each review, the longer this will take."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfSgqeG8O1yo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print (\"Training the random forest...\")\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Initialize a Random Forest classifier with 100 trees\n",
        "forest = RandomForestClassifier(n_estimators = 100) \n",
        "\n",
        "# Fit the forest to the training set, using the bag of words as \n",
        "# features and the sentiment labels as the response variable\n",
        "#\n",
        "# This may take a few minutes to run\n",
        "forest = forest.fit( X_train, y_train )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H-DclpGxT7S-",
        "colab_type": "text"
      },
      "source": [
        "Once we have fitted the model, we can evaluate its accuracy on the validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cGSLmC_PUBf6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "forest.score(X_val,y_val)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPVHiWk2UbEN",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TlcqAQSUbHg",
        "colab_type": "text"
      },
      "source": [
        "We get an accuracy of 84% for the sentiment classification, which is quite ok for the simplicity of the approach."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deU7MDebQkrQ",
        "colab_type": "text"
      },
      "source": [
        "## Creating a Submission\n",
        "All that remains is to run the trained Random Forest on our test set and create a submission file. If you haven't already done so, download testData.tsv from the Data page. This file contains another 25,000 reviews and ids; our task is to predict the sentiment label.\n",
        "\n",
        "Note that when we use the Bag of Words for the test set, we only call \"transform\", not \"fit_transform\" as we did for the training set. In machine learning, you shouldn't use the test set to fit your model, otherwise you run the risk of overfitting. For this reason, we keep the test set off-limits until we are ready to make predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5nFZ7b_GQm3i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Read the test data\n",
        "test = raw_test_data\n",
        "\n",
        "# Verify that there are 25,000 rows and 2 columns\n",
        "print(test.shape)\n",
        "\n",
        "# Create an empty list and append the clean reviews one by one\n",
        "num_reviews = len(test[\"review\"])\n",
        "clean_test_reviews = [] \n",
        "\n",
        "print(\"Cleaning and parsing the test set movie reviews...\\n\")\n",
        "for i in range(0,num_reviews):\n",
        "    if( (i+1) % 1000 == 0 ):\n",
        "        print(\"Review %d of %d\\n\" % (i+1, num_reviews))\n",
        "    clean_review = review_to_words( test[\"review\"][i] )\n",
        "    clean_test_reviews.append( clean_review )\n",
        "\n",
        "# Get a bag of words for the test set, and convert to a numpy array\n",
        "test_data_features = vectorizer.transform(clean_test_reviews)\n",
        "test_data_features = test_data_features.toarray()\n",
        "\n",
        "# Use the random forest to make sentiment label predictions\n",
        "result = grid_clf.predict(test_data_features)\n",
        "\n",
        "# Copy the results to a pandas dataframe with an \"id\" column and\n",
        "# a \"sentiment\" column\n",
        "output = pd.DataFrame( data={\"id\":test[\"id\"], \"sentiment\":result} )\n",
        "\n",
        "# Use pandas to write the comma-separated output file\n",
        "output.to_csv( \"Bag_of_Words_model.csv\", index=False, quoting=3 )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b9-vGLqnQ_Fi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "output.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C_qGZ3kcSjSh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ECEAxwevUqWR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}